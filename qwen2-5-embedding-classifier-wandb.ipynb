{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers accelerate torch opencv-python pillow av scikit-learn\n",
    "!pip install av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import cv2\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB Setup\n",
    "\n",
    "**Important**: Set your WandB API key before running the training cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your WandB API key here\n",
    "WANDB_API_KEY = \"\"\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    wandb.login()\n",
    "    print(\"✓ WandB API key set and logged in\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: WandB API key not set. Please update WANDB_API_KEY variable.\")\n",
    "\n",
    "# WandB configuration\n",
    "WANDB_PROJECT = \"daisee-embedding-classifier\"  # Change this to your project name\n",
    "WANDB_ENTITY = None  # Set to your WandB username/team if needed\n",
    "WANDB_ENABLED = True  # Set to False to disable WandB tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - DAiSEE Dataset\n",
    "BASE_PATH = \"/kaggle/input/daisee/DAiSEE\"\n",
    "TRAIN_DATA_PATH = f\"{BASE_PATH}/DataSet/Train\"\n",
    "VAL_DATA_PATH = f\"{BASE_PATH}/DataSet/Validation\"\n",
    "TEST_DATA_PATH = f\"{BASE_PATH}/DataSet/Test\"\n",
    "\n",
    "TRAIN_LABELS_PATH = f\"{BASE_PATH}/Labels/TrainLabels.csv\"\n",
    "VAL_LABELS_PATH = f\"{BASE_PATH}/Labels/ValidationLabels.csv\"\n",
    "TEST_LABELS_PATH = f\"{BASE_PATH}/Labels/TestLabels.csv\"\n",
    "\n",
    "# Paths - Facial Data (for addressing class imbalance)\n",
    "# UPDATE THIS PATH to your facial data input directory\n",
    "FACIAL_DATA_PATH = \"/kaggle/input/facial-data-mendeley\"\n",
    "FACIAL_DATA_ENABLED = True  # Set to True to use facial data augmentation\n",
    "\n",
    "# Input path for existing embeddings (moved from previous run)\n",
    "EXISTING_EMBEDDINGS_DIR = Path(\"/kaggle/input/qwen-daisee-embeddings/embeddings\")\n",
    "EXISTING_FACIAL_EMBEDDINGS_DIR = Path(\"/kaggle/input/facial-data-embeddings/facial_embeddings\")\n",
    "\n",
    "# Output paths\n",
    "EMBEDDINGS_DIR = Path(\"/kaggle/working/embeddings\")\n",
    "EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_DIR = Path(\"/kaggle/working/models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "FPS = 1  # Sample 1 frame per second (same as inference notebook)\n",
    "\n",
    "# Label categories\n",
    "CATEGORIES = [\"Boredom\", \"Engagement\", \"Confusion\", \"Frustration \"]\n",
    "NUM_CLASSES = 4  # Levels 0-3\n",
    "\n",
    "TRAIN_TRANSFORMER = False\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "HIDDEN_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# ============================================================================\n",
    "# WORKFLOW CONFIGURATION\n",
    "# ============================================================================\n",
    "SKIP_EXTRACTION = False  # Set to True to skip ALL extraction and only train\n",
    "                        # Set to False to enable extraction (configure what to extract below)\n",
    "\n",
    "# Extraction targets (only used when SKIP_EXTRACTION = False)\n",
    "EXTRACT_DAISEE = False      # Extract DAiSEE video embeddings\n",
    "EXTRACT_FACIAL = True     # Extract facial data embeddings (requires FACIAL_DATA_ENABLED = True)\n",
    "\n",
    "# ============================================================================\n",
    "# FACIAL EMBEDDINGS DIRECTORY CONFIGURATION\n",
    "# ============================================================================\n",
    "# Automatically configure facial embeddings directory based on extraction mode\n",
    "if FACIAL_DATA_ENABLED:\n",
    "    if not SKIP_EXTRACTION and EXTRACT_FACIAL:\n",
    "        # Extracting: use working directory for output\n",
    "        FACIAL_EMBEDDINGS_DIR = Path(\"/kaggle/working/facial_embeddings\")\n",
    "        FACIAL_EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "    else:\n",
    "        # Not extracting: use existing embeddings from input\n",
    "        # Check if input directory exists, otherwise fall back to working directory\n",
    "        if EXISTING_FACIAL_EMBEDDINGS_DIR.exists():\n",
    "            FACIAL_EMBEDDINGS_DIR = EXISTING_FACIAL_EMBEDDINGS_DIR\n",
    "        else:\n",
    "            # Input directory doesn't exist - use working directory as fallback\n",
    "            print(f\"⚠ WARNING: EXISTING_FACIAL_EMBEDDINGS_DIR not found: {EXISTING_FACIAL_EMBEDDINGS_DIR}\")\n",
    "            print(f\"  Falling back to working directory\")\n",
    "            FACIAL_EMBEDDINGS_DIR = Path(\"/kaggle/working/facial_embeddings\")\n",
    "            FACIAL_EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "else:\n",
    "    FACIAL_EMBEDDINGS_DIR = None\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION OUTPUT\n",
    "# ============================================================================\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Workflow Mode: {'TRAINING ONLY' if SKIP_EXTRACTION else 'EXTRACTION + TRAINING'}\")\n",
    "print(f\"Facial Data Augmentation: {'ENABLED' if FACIAL_DATA_ENABLED else 'DISABLED'}\")\n",
    "\n",
    "if not SKIP_EXTRACTION:\n",
    "    print(f\"\\nExtraction Targets:\")\n",
    "    print(f\"  - DAiSEE videos: {'YES' if EXTRACT_DAISEE else 'NO'}\")\n",
    "    print(f\"  - Facial data: {'YES' if EXTRACT_FACIAL and FACIAL_DATA_ENABLED else 'NO' if FACIAL_DATA_ENABLED else 'N/A (disabled)'}\")\n",
    "\n",
    "print(f\"\\nDAiSEE Embeddings input: {EXISTING_EMBEDDINGS_DIR}\")\n",
    "if FACIAL_DATA_ENABLED:\n",
    "    print(f\"Facial Data input: {FACIAL_DATA_PATH}\")\n",
    "    if not SKIP_EXTRACTION and EXTRACT_FACIAL:\n",
    "        print(f\"Facial Embeddings output: {FACIAL_EMBEDDINGS_DIR}\")\n",
    "    else:\n",
    "        print(f\"Facial Embeddings input: {FACIAL_EMBEDDINGS_DIR}\")\n",
    "print(f\"New DAiSEE embeddings output: {EMBEDDINGS_DIR}\")\n",
    "print(f\"Models will be saved to: {MODEL_DIR}\")\n",
    "\n",
    "# Check if existing embeddings directory exists\n",
    "if EXISTING_EMBEDDINGS_DIR.exists():\n",
    "    print(f\"\\n✓ Found existing DAiSEE embeddings directory\")\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_dir = EXISTING_EMBEDDINGS_DIR / split\n",
    "        if split_dir.exists():\n",
    "            num_files = len(list(split_dir.glob('*.npy')))\n",
    "            embedding_map_exists = (split_dir / 'embedding_map.pkl').exists()\n",
    "            print(f\"  - {split}: {num_files} embedding files, map: {'✓' if embedding_map_exists else '✗'}\")\n",
    "    \n",
    "    if SKIP_EXTRACTION:\n",
    "        print(f\"\\n⚠ SKIP_EXTRACTION=True: Will use existing embeddings for training\")\n",
    "        print(f\"  Make sure all splits have embedding_map.pkl files!\")\n",
    "else:\n",
    "    if SKIP_EXTRACTION:\n",
    "        print(f\"\\n⚠ WARNING: SKIP_EXTRACTION=True but no existing embeddings found!\")\n",
    "        print(f\"  Please set SKIP_EXTRACTION=False to extract embeddings first\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Existing embeddings directory not found, will extract all from scratch\")\n",
    "# Check facial data if enabled\n",
    "if FACIAL_DATA_ENABLED:\n",
    "    facial_data_path = Path(FACIAL_DATA_PATH)\n",
    "    if facial_data_path.exists():\n",
    "        print(f\"\\n✓ Facial data directory found\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ WARNING: Facial data directory not found at {FACIAL_DATA_PATH}\")\n",
    "        print(f\"  Please update FACIAL_DATA_PATH or set FACIAL_DATA_ENABLED=False\")\n",
    "    \n",
    "    # Check facial embeddings directory\n",
    "    if SKIP_EXTRACTION or not EXTRACT_FACIAL:\n",
    "        if FACIAL_EMBEDDINGS_DIR and FACIAL_EMBEDDINGS_DIR.exists():\n",
    "            print(f\"✓ Existing facial embeddings directory found\")\n",
    "            for category in ['boredom', 'confusion', 'engagement', 'neutral', 'surprise']:\n",
    "                cat_dir = FACIAL_EMBEDDINGS_DIR / category\n",
    "                if cat_dir.exists():\n",
    "                    num_files = len(list(cat_dir.glob('*.npy')))\n",
    "                    map_exists = (cat_dir / 'embedding_map.pkl').exists()\n",
    "                    if num_files > 0 or map_exists:\n",
    "                        print(f\"  - {category}: {num_files} files, map: {'✓' if map_exists else '✗'}\")\n",
    "        else:\n",
    "            print(f\"⚠ WARNING: Facial embeddings directory not found at {FACIAL_EMBEDDINGS_DIR}\")\n",
    "            print(f\"  Please update EXISTING_FACIAL_EMBEDDINGS_DIR or set EXTRACT_FACIAL=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Feature Extraction\n",
    "\n",
    "### Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all label files from DAiSEE\n",
    "train_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "val_df = pd.read_csv(VAL_LABELS_PATH)\n",
    "test_df = pd.read_csv(TEST_LABELS_PATH)\n",
    "\n",
    "print(\"DAiSEE Dataset:\")\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Total: {len(train_df) + len(val_df) + len(test_df)}\")\n",
    "\n",
    "# Analyze class distribution in DAiSEE\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DAiSEE CLASS DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "for category in CATEGORIES:\n",
    "    category_clean = category.strip()\n",
    "    print(f\"\\n{category_clean}:\")\n",
    "    train_dist = train_df[category].value_counts().sort_index()\n",
    "    for level in range(NUM_CLASSES):\n",
    "        count = train_dist.get(level, 0)\n",
    "        pct = (count / len(train_df)) * 100\n",
    "        print(f\"  Level {level}: {count:4d} samples ({pct:5.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Facial Data (Optional - for Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_data_dfs = {}\n",
    "\n",
    "if FACIAL_DATA_ENABLED:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING FACIAL DATA FOR CLASS IMBALANCE AUGMENTATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    facial_data_path = Path(FACIAL_DATA_PATH)\n",
    "    \n",
    "    # Expected structure: FACIAL_DATA_PATH contains CSV files for each emotion\n",
    "    # e.g., boring.csv, confused.csv, happiness.csv (for engagement), neutral.csv, surprise.csv\n",
    "    # Each CSV should have columns: image_path, label (0-3 for intensity levels)\n",
    "    \n",
    "    emotion_mapping = {\n",
    "        'Boredom': ['boring.csv', 'boredom.csv'],\n",
    "        'Engagement': ['happiness.csv', 'happy.csv', 'engaged.csv'],\n",
    "        'Confusion': ['confused.csv', 'confusion.csv'],\n",
    "        'Frustration': ['surprise.csv', 'frustrated.csv', 'frustration.csv']\n",
    "    }\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        category_clean = category.strip()\n",
    "        \n",
    "        # Try to find matching CSV file\n",
    "        csv_found = None\n",
    "        for possible_name in emotion_mapping.get(category_clean, []):\n",
    "            csv_path = facial_data_path / possible_name\n",
    "            if csv_path.exists():\n",
    "                csv_found = csv_path\n",
    "                break\n",
    "        \n",
    "        if csv_found:\n",
    "            try:\n",
    "                # Read CSV without headers (raw pixel data)\n",
    "                df = pd.read_csv(csv_found, header=None)\n",
    "                \n",
    "                # Add metadata columns\n",
    "                # image_id: unique identifier for each image (row index)\n",
    "                # emotion: the category this CSV represents\n",
    "                # label: intensity level (assign all to level 2 for moderate intensity)\n",
    "                df.insert(0, 'image_id', range(len(df)))\n",
    "                df.insert(1, 'emotion', category_clean)\n",
    "                df.insert(2, 'label', 2)  # Assign all facial data to intensity level 2\n",
    "                \n",
    "                facial_data_dfs[category_clean] = df\n",
    "                \n",
    "                print(f\"\\n✓ Loaded {category_clean} from {csv_found.name}\")\n",
    "                print(f\"  Total samples: {len(df)}\")\n",
    "                print(f\"  Image columns: {len(df.columns) - 3}\")  # Subtract metadata columns\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n✗ Error loading {category_clean}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"\\n⚠ No facial data found for {category_clean}\")\n",
    "            print(f\"  Tried: {emotion_mapping.get(category_clean, [])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Loaded facial data for {len(facial_data_dfs)}/{len(CATEGORIES)} categories\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\nFacial data augmentation disabled (FACIAL_DATA_ENABLED=False)\")\n",
    "    print(\"Using DAiSEE dataset only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_path(clip_id, data_path):\n",
    "    \"\"\"\n",
    "    Find the full path to a video file given its ClipID.\n",
    "    Structure: DataSet/Split/Subject/VideoFolder/VideoFile.avi\n",
    "    \"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    clip_name = clip_id.replace('.avi', '')\n",
    "    subject_id = clip_name[:6]\n",
    "    video_path = data_path / subject_id / clip_name / clip_id\n",
    "    return video_path\n",
    "\n",
    "\n",
    "# Test video path\n",
    "test_clip = train_df.iloc[0]['ClipID']\n",
    "test_path = find_video_path(test_clip, TRAIN_DATA_PATH)\n",
    "print(f\"Test video: {test_clip}\")\n",
    "print(f\"Path exists: {test_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Qwen2.5-VL Model for Embedding Extraction\n",
    "\n",
    "**Note**: Only needed if `SKIP_EXTRACTION = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_EXTRACTION and (EXTRACT_DAISEE or EXTRACT_FACIAL):\n",
    "    print(\"Loading Qwen2.5-VL model...\")\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    vision_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    vision_model.eval()\n",
    "    print(f\"Model loaded on {DEVICE}\")\n",
    "else:\n",
    "    if SKIP_EXTRACTION:\n",
    "        print(\"Skipping model loading (SKIP_EXTRACTION=True)\")\n",
    "    else:\n",
    "        print(\"Skipping model loading (no extraction targets enabled)\")\n",
    "    processor = None\n",
    "    vision_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Embeddings Function\n",
    "\n",
    "**Note**: Only needed if `SKIP_EXTRACTION = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_EXTRACTION:\n",
    "    def extract_video_embeddings(video_path, processor, model, fps=1):\n",
    "        \"\"\"\n",
    "        Extract embeddings from video using Qwen model.\n",
    "        Uses the same video processing approach as inference notebook with fps parameter.\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "            processor: Qwen processor\n",
    "            model: Qwen model\n",
    "            fps: Frames per second for temporal sampling (same as inference notebook)\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (num_frames, embedding_dim)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a dummy message structure to use apply_chat_template for video processing\n",
    "            # This ensures fps parameter is handled correctly like in inference notebook\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"video\", \"video\": str(video_path)},\n",
    "                        {\"type\": \"text\", \"text\": \"Analyze this video.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process video with fps parameter (same as inference notebook)\n",
    "            with torch.no_grad():\n",
    "                inputs = processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    fps=fps,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=True,\n",
    "                    return_dict=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # Move to device\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Forward pass through model to get hidden states\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Extract embeddings from the last hidden state\n",
    "                # Shape: (batch_size, sequence_length, hidden_dim)\n",
    "                hidden_states = outputs.hidden_states[-1]\n",
    "                \n",
    "                # Remove batch dimension but keep temporal structure\n",
    "                # This gives us (sequence_length, hidden_dim) for temporal modeling\n",
    "                embeddings = hidden_states.squeeze(0)\n",
    "                \n",
    "                # Convert to CPU numpy\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "            \n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting embeddings from {video_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "\n",
    "    # Test embedding extraction\n",
    "    if test_path.exists():\n",
    "        print(\"\\nTesting embedding extraction...\")\n",
    "        test_embeddings = extract_video_embeddings(test_path, processor, vision_model, fps=FPS)\n",
    "        if test_embeddings is not None:\n",
    "            print(f\"Embeddings shape: {test_embeddings.shape}\")\n",
    "            print(f\"Number of frames: {test_embeddings.shape[0]}\")\n",
    "            print(f\"Embedding dimension: {test_embeddings.shape[1]}\")\n",
    "        else:\n",
    "            print(\"Failed to extract embeddings\")\n",
    "else:\n",
    "    print(\"Skipping embedding extraction function (SKIP_EXTRACTION=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Save Embeddings for All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_facial_embeddings(facial_data_dfs, processor, model):\n",
    "    \"\"\"\n",
    "    Extract embeddings from facial data CSV files containing raw pixel arrays.\n",
    "    \n",
    "    Args:\n",
    "        facial_data_dfs: Dictionary of DataFrames with pixel data\n",
    "        processor: Qwen processor\n",
    "        model: Qwen vision model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping category -> (image_id -> embedding_path)\n",
    "    \"\"\"\n",
    "    if not facial_data_dfs:\n",
    "        print(\"No facial data to process\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTRACTING FACIAL DATA EMBEDDINGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    facial_embedding_maps = {}\n",
    "    \n",
    "    for category, df in facial_data_dfs.items():\n",
    "        print(f\"\\nProcessing {category} ({len(df):,} images)...\")\n",
    "        \n",
    "        category_dir = FACIAL_EMBEDDINGS_DIR / category.lower()\n",
    "        category_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        embedding_map = {}\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=category):\n",
    "            try:\n",
    "                # Extract pixel values (all columns except metadata: image_id, emotion, label)\n",
    "                # Pixel columns start from index 3 onwards\n",
    "                pixels = row.iloc[3:].values.astype(np.uint8)\n",
    "                \n",
    "                # Verify we have the right number of pixels\n",
    "                if len(pixels) != 256 * 256 * 3:\n",
    "                    print(f\"\\nWarning: Image {row['image_id']} has {len(pixels)} pixels, expected {256*256*3}\")\n",
    "                    continue\n",
    "                \n",
    "                # Reshape to 256x256x3 RGB image\n",
    "                image = pixels.reshape(256, 256, 3)\n",
    "                \n",
    "                # Convert to PIL Image for processor\n",
    "                from PIL import Image\n",
    "                pil_image = Image.fromarray(image)\n",
    "                \n",
    "                # Save image temporarily (Qwen2.5-VL requires file path or URL)\n",
    "                temp_image_path = FACIAL_EMBEDDINGS_DIR / f\"temp_{idx}.jpg\"\n",
    "                pil_image.save(temp_image_path)\n",
    "                \n",
    "                try:\n",
    "                    # Process with Qwen using message-based API (same as video extraction)\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\": str(temp_image_path)},\n",
    "                                {\"type\": \"text\", \"text\": \"Analyze this image.\"}\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                    \n",
    "                    # Extract embeddings\n",
    "                    with torch.no_grad():\n",
    "                        inputs = processor.apply_chat_template(\n",
    "                            messages,\n",
    "                            add_generation_prompt=True,\n",
    "                            tokenize=True,\n",
    "                            return_dict=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                        \n",
    "                        # Move to device\n",
    "                        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        outputs = model(**inputs, output_hidden_states=True)\n",
    "                        hidden_states = outputs.hidden_states[-1]\n",
    "                        \n",
    "                        # Global average pooling over spatial dimensions\n",
    "                        # Shape: (batch, seq_len, hidden_dim) -> (hidden_dim,)\n",
    "                        embedding = hidden_states.mean(dim=1).squeeze(0)\n",
    "                        embedding = embedding.cpu().numpy()\n",
    "                finally:\n",
    "                    # Clean up temporary image\n",
    "                    if temp_image_path.exists():\n",
    "                        temp_image_path.unlink()\n",
    "                \n",
    "                # Save embedding\n",
    "                image_id = row['image_id']\n",
    "                save_path = category_dir / f\"{image_id}.npy\"\n",
    "                np.save(save_path, embedding)\n",
    "                \n",
    "                embedding_map[image_id] = str(save_path)\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {category} image {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        facial_embedding_maps[category] = embedding_map\n",
    "        print(f\"✓ Extracted {len(embedding_map):,} embeddings for {category}\")\n",
    "        \n",
    "        # Save embedding map\n",
    "        map_path = category_dir / \"embedding_map.pkl\"\n",
    "        with open(map_path, 'wb') as f:\n",
    "            pickle.dump(embedding_map, f)\n",
    "        print(f\"  Saved to {category_dir}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Facial embeddings extraction complete\")\n",
    "    print(f\"Total categories: {len(facial_embedding_maps)}\")\n",
    "    total_embeddings = sum(len(m) for m in facial_embedding_maps.values())\n",
    "    print(f\"Total embeddings: {total_embeddings:,}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return facial_embedding_maps\n",
    "\n",
    "\n",
    "# Extract facial embeddings if enabled\n",
    "facial_embedding_maps = {}\n",
    "\n",
    "if FACIAL_DATA_ENABLED and not SKIP_EXTRACTION and EXTRACT_FACIAL and facial_data_dfs:\n",
    "    if processor is not None and vision_model is not None:\n",
    "        facial_embedding_maps = extract_facial_embeddings(facial_data_dfs, processor, vision_model)\n",
    "    else:\n",
    "        print(\"\\n⚠ WARNING: Cannot extract facial embeddings - model not loaded\")\n",
    "        print(\"  Set SKIP_EXTRACTION=False and EXTRACT_FACIAL=True to extract embeddings\")\n",
    "elif FACIAL_DATA_ENABLED and (SKIP_EXTRACTION or not EXTRACT_FACIAL):\n",
    "    # Load existing facial embeddings\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING EXISTING FACIAL EMBEDDINGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for category in facial_data_dfs.keys():\n",
    "        category_dir = FACIAL_EMBEDDINGS_DIR / category.lower()\n",
    "        map_path = category_dir / \"embedding_map.pkl\"\n",
    "        \n",
    "        if map_path.exists():\n",
    "            try:\n",
    "                with open(map_path, 'rb') as f:\n",
    "                    embedding_map = pickle.load(f)\n",
    "                facial_embedding_maps[category] = embedding_map\n",
    "                print(f\"✓ Loaded {len(embedding_map):,} embeddings for {category}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Error loading {category} embeddings: {e}\")\n",
    "        else:\n",
    "            print(f\"⚠ No embedding map found for {category} at {map_path}\")\n",
    "    \n",
    "    if facial_embedding_maps:\n",
    "        total = sum(len(m) for m in facial_embedding_maps.values())\n",
    "        print(f\"\\nTotal facial embeddings loaded: {total:,}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    if not FACIAL_DATA_ENABLED:\n",
    "        print(\"\\nSkipping facial embeddings (FACIAL_DATA_ENABLED=False)\")\n",
    "    elif not EXTRACT_FACIAL:\n",
    "        print(\"\\nSkipping facial embeddings (EXTRACT_FACIAL=False)\")\n",
    "    else:\n",
    "        print(\"\\nSkipping facial embeddings (no data loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Facial Data Embeddings\n",
    "\n",
    "**Note**: Only needed if `FACIAL_DATA_ENABLED = True` and `SKIP_EXTRACTION = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_embeddings(df, data_path, split_name):\n",
    "    \"\"\"\n",
    "    Extract embeddings for all videos in a dataset split and save to disk.\n",
    "    Resumes from existing embeddings if available.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with video labels\n",
    "        data_path: Path to video data\n",
    "        split_name: Name of split (train/val/test)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping ClipID to embedding filepath\n",
    "    \"\"\"\n",
    "    # Check for existing embeddings\n",
    "    existing_split_dir = EXISTING_EMBEDDINGS_DIR / split_name\n",
    "    output_split_dir = EMBEDDINGS_DIR / split_name\n",
    "    output_split_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    embedding_map = {}\n",
    "    failed_videos = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Load existing embedding map if available\n",
    "    existing_map_path = existing_split_dir / \"embedding_map.pkl\" if existing_split_dir.exists() else None\n",
    "    if existing_map_path and existing_map_path.exists():\n",
    "        try:\n",
    "            with open(existing_map_path, 'rb') as f:\n",
    "                existing_map = pickle.load(f)\n",
    "            print(f\"✓ Loaded existing embedding map with {len(existing_map)} entries\")\n",
    "            \n",
    "            # Copy existing embeddings to output directory and update paths\n",
    "            for clip_id, old_path in existing_map.items():\n",
    "                old_path = Path(old_path)\n",
    "                if old_path.exists():\n",
    "                    # Copy to output directory\n",
    "                    new_path = output_split_dir / old_path.name\n",
    "                    if not new_path.exists():\n",
    "                        shutil.copy2(old_path, new_path)\n",
    "                    embedding_map[clip_id] = str(new_path)\n",
    "                    skipped_count += 1\n",
    "            \n",
    "            print(f\"✓ Copied {skipped_count} existing embeddings to working directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error loading existing embeddings: {e}\")\n",
    "            print(f\"  Will extract all embeddings from scratch\")\n",
    "    \n",
    "    print(f\"\\nExtracting embeddings for {split_name} set ({len(df)} videos)...\")\n",
    "    if skipped_count > 0:\n",
    "        print(f\"Resuming from {skipped_count} existing embeddings\")\n",
    "    \n",
    "    extracted_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        clip_id = row['ClipID']\n",
    "        \n",
    "        # Skip if already in embedding map\n",
    "        if clip_id in embedding_map:\n",
    "            continue\n",
    "        \n",
    "        video_path = find_video_path(clip_id, data_path)\n",
    "        \n",
    "        if not video_path.exists():\n",
    "            failed_videos.append(clip_id)\n",
    "            continue\n",
    "        \n",
    "        # Extract embeddings directly from video (using fps parameter like inference notebook)\n",
    "        try:\n",
    "            embeddings = extract_video_embeddings(video_path, processor, vision_model, fps=FPS)\n",
    "            \n",
    "            if embeddings is None:\n",
    "                failed_videos.append(clip_id)\n",
    "                continue\n",
    "            \n",
    "            # Save embeddings\n",
    "            save_path = output_split_dir / f\"{clip_id.replace('.avi', '')}.npy\"\n",
    "            np.save(save_path, embeddings)\n",
    "            \n",
    "            embedding_map[clip_id] = str(save_path)\n",
    "            extracted_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {clip_id}: {e}\")\n",
    "            failed_videos.append(clip_id)\n",
    "        \n",
    "        # Clear memory periodically\n",
    "        if (extracted_count + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    total_embeddings = len(embedding_map)\n",
    "    print(f\"\\n✓ Total embeddings: {total_embeddings}/{len(df)} videos\")\n",
    "    print(f\"  - Existing: {skipped_count}\")\n",
    "    print(f\"  - Newly extracted: {extracted_count}\")\n",
    "    if failed_videos:\n",
    "        print(f\"  - Failed: {len(failed_videos)} videos\")\n",
    "    \n",
    "    # Save embedding map\n",
    "    map_path = output_split_dir / \"embedding_map.pkl\"\n",
    "    with open(map_path, 'wb') as f:\n",
    "        pickle.dump(embedding_map, f)\n",
    "    print(f\"✓ Embedding map saved to {map_path}\")\n",
    "    \n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_EXTRACTION or not EXTRACT_DAISEE:\n",
    "    # Skip extraction and copy existing embeddings to working directory\n",
    "    print(\"=\"*80)\n",
    "    if SKIP_EXTRACTION:\n",
    "        print(\"SKIPPING EXTRACTION - USING EXISTING EMBEDDINGS\")\n",
    "    else:\n",
    "        print(\"SKIPPING DAISEE EXTRACTION - USING EXISTING EMBEDDINGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Copy existing embeddings to working directory\n",
    "    for split, data_path in [\n",
    "        ('train', TRAIN_DATA_PATH),\n",
    "        ('validation', VAL_DATA_PATH),\n",
    "        ('test', TEST_DATA_PATH)\n",
    "    ]:\n",
    "        existing_split_dir = EXISTING_EMBEDDINGS_DIR / split\n",
    "        output_split_dir = EMBEDDINGS_DIR / split\n",
    "        output_split_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if not existing_split_dir.exists():\n",
    "            print(f\"\\n✗ {split}: Existing embeddings not found at {existing_split_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Load existing embedding map\n",
    "        existing_map_path = existing_split_dir / \"embedding_map.pkl\"\n",
    "        if not existing_map_path.exists():\n",
    "            print(f\"\\n✗ {split}: No embedding map found at {existing_map_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(existing_map_path, 'rb') as f:\n",
    "            existing_map = pickle.load(f)\n",
    "        \n",
    "        print(f\"\\n{split.upper()}: Processing {len(existing_map)} embeddings...\")\n",
    "        \n",
    "        # Copy embeddings and create new map\n",
    "        new_map = {}\n",
    "        copied_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for clip_id, old_path in tqdm(existing_map.items(), desc=f\"Copying {split}\"):\n",
    "            old_path = Path(old_path)\n",
    "            new_path = output_split_dir / old_path.name\n",
    "            \n",
    "            # Check if file already exists in working directory\n",
    "            if new_path.exists():\n",
    "                new_map[clip_id] = str(new_path)\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Try to find the source file\n",
    "            source_path = None\n",
    "            \n",
    "            # First, try the path from the map (might be input or previous working dir)\n",
    "            if old_path.exists():\n",
    "                source_path = old_path\n",
    "            # If not found, try to construct path from input directory\n",
    "            elif existing_split_dir.exists():\n",
    "                potential_source = existing_split_dir / old_path.name\n",
    "                if potential_source.exists():\n",
    "                    source_path = potential_source\n",
    "            \n",
    "            # Copy file if source found\n",
    "            if source_path:\n",
    "                shutil.copy2(source_path, new_path)\n",
    "                new_map[clip_id] = str(new_path)\n",
    "                copied_count += 1\n",
    "            else:\n",
    "                # File not found anywhere - skip this embedding\n",
    "                if idx == 0:  # Only warn for first few to avoid spam\n",
    "                    print(f\"\\n  ⚠ Warning: Embedding not found for {clip_id}\")\n",
    "                    print(f\"     Tried: {old_path} and {existing_split_dir / old_path.name if existing_split_dir.exists() else 'N/A'}\")\n",
    "        \n",
    "        # Save new embedding map\n",
    "        new_map_path = output_split_dir / \"embedding_map.pkl\"\n",
    "        with open(new_map_path, 'wb') as f:\n",
    "            pickle.dump(new_map, f)\n",
    "        \n",
    "        print(f\"  ✓ Total embeddings in map: {len(new_map)}\")\n",
    "        print(f\"  ✓ Newly copied: {copied_count}\")\n",
    "        print(f\"  ✓ Already existed: {skipped_count}\")\n",
    "        print(f\"  ✓ Saved map to {new_map_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "elif EXTRACT_DAISEE:\n",
    "    # Run DAiSEE extraction\n",
    "\n",
    "    print(\"DAISEE EMBEDDING EXTRACTION\")\n",
    "    # Run extraction\n",
    "    print(\"=\"*80)\n",
    "    print(\"EMBEDDING EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    train_embedding_map = extract_and_save_embeddings(train_df, TRAIN_DATA_PATH, 'train')\n",
    "    val_embedding_map = extract_and_save_embeddings(val_df, VAL_DATA_PATH, 'validation')\n",
    "    print(\"\\nDAISEE EXTRACTION COMPLETE\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTRACTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n⚠ No DAiSEE extraction performed (EXTRACT_DAISEE=False)\")\n",
    "    print(\"  Ensure existing embeddings are available for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free GPU Memory (optional - can restart kernel if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_EXTRACTION:\n",
    "    # Free up memory by deleting the large vision model\n",
    "    del vision_model\n",
    "    del processor\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Vision model removed from memory\")\n",
    "else:\n",
    "    print(\"No vision model to remove (was never loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Classifier Training\n",
    "\n",
    "### Dataset Class for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading pre-computed embeddings.\n",
    "    Supports both DAiSEE video embeddings and facial data image embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, embedding_map, category, facial_data_df=None, facial_embedding_map=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with labels (DAiSEE)\n",
    "            embedding_map: Dictionary mapping ClipID to embedding filepath (DAiSEE)\n",
    "            category: Category to predict (e.g., 'Boredom')\n",
    "            facial_data_df: Optional DataFrame with facial pixel data (columns: image_id, emotion, label, ...)\n",
    "            facial_embedding_map: Optional dictionary mapping facial image_id to embedding paths\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.embedding_map = embedding_map\n",
    "        self.category = category\n",
    "        self.facial_data_df = facial_data_df\n",
    "        self.facial_embedding_map = facial_embedding_map\n",
    "        \n",
    "        # Filter DAiSEE to only include videos with embeddings\n",
    "        self.valid_indices = [\n",
    "            i for i in range(len(df)) \n",
    "            if df.iloc[i]['ClipID'] in embedding_map\n",
    "        ]\n",
    "        \n",
    "        # Debug: Check if no valid samples found\n",
    "        if len(self.valid_indices) == 0 and len(df) > 0:\n",
    "            print(f\"  ⚠ WARNING: No embeddings found for any videos!\")\n",
    "            print(f\"     DataFrame has {len(df)} rows\")\n",
    "            print(f\"     Embedding map has {len(embedding_map)} entries\")\n",
    "            if len(df) > 0 and len(embedding_map) > 0:\n",
    "                print(f\"     Sample ClipID from DataFrame: {df.iloc[0]['ClipID']}\")\n",
    "                print(f\"     Sample key from embedding map: {list(embedding_map.keys())[0]}\")\n",
    "        \n",
    "        # Add facial data indices if available\n",
    "        self.facial_indices = []\n",
    "        if facial_data_df is not None and facial_embedding_map is not None:\n",
    "            # Filter facial data: only include samples with matching emotion and valid embeddings\n",
    "            for i in range(len(facial_data_df)):\n",
    "                row = facial_data_df.iloc[i]\n",
    "                \n",
    "                # Check if this image belongs to the current category\n",
    "                if row.get('emotion') == category:\n",
    "                    img_id = row['image_id']\n",
    "                    \n",
    "                    # Check if embedding exists\n",
    "                    if img_id in facial_embedding_map:\n",
    "                        self.facial_indices.append(i)\n",
    "        \n",
    "        total_samples = len(self.valid_indices) + len(self.facial_indices)\n",
    "        print(f\"Dataset: {total_samples} total samples for {category}\")\n",
    "        print(f\"  - DAiSEE: {len(self.valid_indices)} samples\")\n",
    "        if self.facial_indices:\n",
    "            print(f\"  - Facial Data: {len(self.facial_indices)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices) + len(self.facial_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine if this is a DAiSEE or Facial data sample\n",
    "        if idx < len(self.valid_indices):\n",
    "            # DAiSEE sample\n",
    "            actual_idx = self.valid_indices[idx]\n",
    "            row = self.df.iloc[actual_idx]\n",
    "            \n",
    "            clip_id = row['ClipID']\n",
    "            embedding_path = self.embedding_map[clip_id]\n",
    "            \n",
    "            # Load embedding\n",
    "            embedding = np.load(embedding_path)\n",
    "            \n",
    "            # Get label\n",
    "            label = int(row[self.category])\n",
    "        else:\n",
    "            # Facial data sample\n",
    "            facial_idx = idx - len(self.valid_indices)\n",
    "            actual_facial_idx = self.facial_indices[facial_idx]\n",
    "            row = self.facial_data_df.iloc[actual_facial_idx]\n",
    "            \n",
    "            # Load embedding using image_id\n",
    "            img_id = row['image_id']\n",
    "            embedding_path = self.facial_embedding_map[img_id]\n",
    "            \n",
    "            # Load embedding\n",
    "            embedding = np.load(embedding_path)\n",
    "            \n",
    "            # Get label (assigned during loading, typically level 2)\n",
    "            label = int(row['label'])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        embedding = torch.FloatTensor(embedding)\n",
    "        label = torch.LongTensor([label])[0]\n",
    "        \n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Architectures\n",
    "\n",
    "#### 1. Simple MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Multi-Layer Perceptron classifier.\n",
    "    Works with pooled (1D) embeddings from videos or single frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=HIDDEN_DIM, num_classes=4, dropout=DROPOUT):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, embedding_dim) - pre-pooled embeddings\n",
    "        # No temporal dimension since embeddings are already averaged over video frames\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Transformer Encoder Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    NOTE: This classifier is designed for temporal (video) data with multiple frames.\n",
    "    Since we're using pooled embeddings (one vector per video), this is simplified\n",
    "    to work like MLP but kept for compatibility.\n",
    "    \n",
    "    For true temporal modeling, use unpooled embeddings with shape (num_frames, embedding_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_heads=8, num_layers=2, hidden_dim=512, num_classes=4, dropout=0.3):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        # Since we have pooled embeddings, this becomes a simple MLP with extra layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, embedding_dim) - pre-pooled embeddings\n",
    "        # No transformer needed since temporal information is already pooled\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for embeddings, labels in dataloader:\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifiers for Each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(category, model_type='mlp'):\n",
    "    \"\"\"\n",
    "    Train a classifier for a specific category.\n",
    "    \n",
    "    Args:\n",
    "        category: Emotion category to predict\n",
    "        model_type: 'mlp' or 'transformer'\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_type.upper()} classifier for {category}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load DAiSEE embedding maps\n",
    "    train_map_path = EMBEDDINGS_DIR / 'train' / 'embedding_map.pkl'\n",
    "    val_map_path = EMBEDDINGS_DIR / 'validation' / 'embedding_map.pkl'\n",
    "    test_map_path = EMBEDDINGS_DIR / 'test' / 'embedding_map.pkl'\n",
    "    \n",
    "    # Validate embedding maps exist\n",
    "    if not train_map_path.exists():\n",
    "        raise FileNotFoundError(f\"Training embedding map not found: {train_map_path}\")\n",
    "    if not val_map_path.exists():\n",
    "        raise FileNotFoundError(f\"Validation embedding map not found: {val_map_path}\")\n",
    "    if not test_map_path.exists():\n",
    "        raise FileNotFoundError(f\"Test embedding map not found: {test_map_path}\")\n",
    "    \n",
    "    with open(train_map_path, 'rb') as f:\n",
    "        train_map = pickle.load(f)\n",
    "    with open(val_map_path, 'rb') as f:\n",
    "        val_map = pickle.load(f)\n",
    "    with open(test_map_path, 'rb') as f:\n",
    "        test_map = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded embedding maps:\")\n",
    "    print(f\"  Train: {len(train_map)} embeddings\")\n",
    "    print(f\"  Val: {len(val_map)} embeddings\")\n",
    "    print(f\"  Test: {len(test_map)} embeddings\")\n",
    "    \n",
    "    # Validate that embedding files actually exist\n",
    "    missing_train = [k for k, v in train_map.items() if not Path(v).exists()]\n",
    "    if missing_train:\n",
    "        print(f\"  ⚠ WARNING: {len(missing_train)} train embeddings not found on disk!\")\n",
    "        if len(missing_train) <= 5:\n",
    "            for k in missing_train[:5]:\n",
    "                print(f\"     Missing: {k} -> {train_map[k]}\")\n",
    "    \n",
    "    # Load facial data embedding maps if available\n",
    "    facial_train_map = None\n",
    "    facial_train_df = None\n",
    "    \n",
    "    category_clean = category.strip()\n",
    "    if FACIAL_DATA_ENABLED and category_clean in facial_data_dfs:\n",
    "        if category_clean in facial_embedding_maps:\n",
    "            facial_train_map = facial_embedding_maps[category_clean]\n",
    "            facial_train_df = facial_data_dfs[category_clean]\n",
    "            print(f\"✓ Using facial data augmentation for {category_clean}\")\n",
    "            print(f\"  Facial samples: {len(facial_train_map):,}\")\n",
    "        else:\n",
    "            print(f\"⚠ No facial embeddings found for {category_clean}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EmbeddingDataset(\n",
    "        train_df, train_map, category,\n",
    "        facial_data_df=facial_train_df,\n",
    "        facial_embedding_map=facial_train_map\n",
    "    )\n",
    "    val_dataset = EmbeddingDataset(val_df, val_map, category)\n",
    "    test_dataset = EmbeddingDataset(test_df, test_map, category)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Get embedding dimension from first sample\n",
    "    sample_embedding, _ = train_dataset[0]\n",
    "    \n",
    "    # Check if embeddings are 1D (pooled) or 2D (temporal)\n",
    "    if sample_embedding.dim() == 1:\n",
    "        embedding_dim = sample_embedding.shape[0]\n",
    "        print(f\"Using pooled embeddings (1D)\")\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    else:\n",
    "        embedding_dim = sample_embedding.shape[1]\n",
    "        print(f\"Using temporal embeddings (2D)\")\n",
    "        print(f\"Shape: {sample_embedding.shape}\")\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    # Create model\n",
    "    if model_type == 'mlp':\n",
    "        model = MLPClassifier(input_dim=embedding_dim, num_classes=NUM_CLASSES)\n",
    "    else:\n",
    "        model = TransformerClassifier(input_dim=embedding_dim, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    # Initialize WandB run\n",
    "    if WANDB_ENABLED:\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_ENTITY,\n",
    "            name=f\"{model_type}_{category_clean}_{FACIAL_DATA_ENABLED}_{HIDDEN_DIM}_{DROPOUT}\",\n",
    "            config={\n",
    "                \"category\": category_clean,\n",
    "                \"model_type\": model_type,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"num_epochs\": NUM_EPOCHS,\n",
    "                \"embedding_dim\": embedding_dim,\n",
    "                \"num_classes\": NUM_CLASSES,\n",
    "                \"facial_data_enabled\": FACIAL_DATA_ENABLED,\n",
    "                \"hidden_dim\": HIDDEN_DIM,\n",
    "                \"dropout\": DROPOUT,\n",
    "                \"train_samples\": len(train_dataset),\n",
    "                \"val_samples\": len(val_dataset),\n",
    "                \"test_samples\": len(test_dataset),\n",
    "            },\n",
    "            reinit=True\n",
    "        )\n",
    "        wandb.watch(model, log=\"all\", log_freq=10)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        scheduler.step(val_metrics['f1'])\n",
    "        \n",
    "        # Log metrics to WandB after every epoch\n",
    "        if WANDB_ENABLED:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_loss\": val_metrics['loss'],\n",
    "                \"val_accuracy\": val_metrics['accuracy'] * 100,\n",
    "                \"val_f1\": val_metrics['f1'],\n",
    "                \"val_precision\": val_metrics['precision'],\n",
    "                \"val_recall\": val_metrics['recall'],\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            })\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']*100:.2f}%\")\n",
    "            print(f\"  Val F1: {val_metrics['f1']:.4f} | Val Precision: {val_metrics['precision']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            save_path = MODEL_DIR / f\"{model_type}_{category_clean}_best.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': best_val_f1,\n",
    "            }, save_path)\n",
    "    \n",
    "    print(f\"\\nBest model from epoch {best_epoch} with Val F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    checkpoint = torch.load(MODEL_DIR / f\"{model_type}_{category_clean}_best.pth\", map_location='cpu', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_metrics = evaluate(model, test_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(f\"  Accuracy: {test_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_metrics['labels'], test_metrics['predictions'], labels=[0, 1, 2, 3])\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_results = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    metrics = train_classifier(category, model_type='mlp')\n",
    "    mlp_results[category] = metrics\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Transformer Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TRANSFORMER:\n",
    "    transformer_results = {}\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        metrics = train_classifier(category, model_type='transformer')\n",
    "        transformer_results[category] = metrics\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison\n",
    "summary_data = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    mlp_metrics = mlp_results[category]\n",
    "    trans_metrics = transformer_results[category]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Category': category.strip(),\n",
    "        'MLP Accuracy': f\"{mlp_metrics['accuracy']*100:.2f}%\",\n",
    "        'MLP F1': f\"{mlp_metrics['f1']:.4f}\",\n",
    "        'Transformer Accuracy': f\"{trans_metrics['accuracy']*100:.2f}%\",\n",
    "        'Transformer F1': f\"{trans_metrics['f1']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(MODEL_DIR / 'results_summary.csv', index=False)\n",
    "print(f\"\\n✓ Results saved to {MODEL_DIR / 'results_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "This two-stage approach offers several advantages:\n",
    "\n",
    "1. **Efficiency**: Extract embeddings once, train multiple classifiers\n",
    "2. **Speed**: Small classifiers train in seconds/minutes vs hours\n",
    "3. **Experimentation**: Easy to try different architectures\n",
    "4. **Memory**: Lower memory requirements during training\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different pooling strategies (attention-based, LSTM)\n",
    "- Experiment with class weights for imbalanced data\n",
    "- Fine-tune the vision model end-to-end\n",
    "- Ensemble multiple classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Models for Deployment\n",
    "\n",
    "Package trained classifiers into a zip file for easy deployment to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Paths\n",
    "EXPORT_DIR = Path(\"/kaggle/working/export\")\n",
    "OUTPUT_ZIP = Path(\"/kaggle/working/deployment_models.zip\")\n",
    "\n",
    "def export_models_for_deployment():\n",
    "    \"\"\"Export trained classifiers for deployment\"\"\"\n",
    "    \n",
    "    if not MODEL_DIR.exists():\n",
    "        print(f\"Error: Models directory not found at {MODEL_DIR}\")\n",
    "        return\n",
    "    \n",
    "    # Create temporary export directory\n",
    "    EXPORT_DIR.mkdir(exist_ok=True)\n",
    "    classifiers_dir = EXPORT_DIR / \"classifiers\"\n",
    "    classifiers_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORTING MODELS FOR DEPLOYMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    exported_count = 0\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        category_clean = category.strip()\n",
    "        \n",
    "        # Look for MLP classifier (best performing)\n",
    "        checkpoint_path = MODEL_DIR / f\"mlp_{category_clean}_best.pth\"\n",
    "        \n",
    "        if checkpoint_path.exists():\n",
    "            # Copy to export directory\n",
    "            dest_path = classifiers_dir / f\"mlp_{category_clean}_best.pth\"\n",
    "            shutil.copy2(checkpoint_path, dest_path)\n",
    "            \n",
    "            # Load and print info\n",
    "            try:\n",
    "                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {e}\")\n",
    "                continue\n",
    "            val_f1 = checkpoint.get('val_f1', 'N/A')\n",
    "            epoch = checkpoint.get('epoch', 'N/A')\n",
    "            \n",
    "            print(f\"\\n✓ Exported {category_clean}:\")\n",
    "            print(f\"  - Epoch: {epoch}\")\n",
    "            print(f\"  - Val F1: {val_f1:.4f if isinstance(val_f1, float) else val_f1}\")\n",
    "            print(f\"  - Size: {dest_path.stat().st_size / 1024:.1f} KB\")\n",
    "            \n",
    "            exported_count += 1\n",
    "        else:\n",
    "            print(f\"\\n✗ Not found: {category_clean}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Exported {exported_count}/{len(CATEGORIES)} classifiers\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_path = EXPORT_DIR / \"README.txt\"\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(\"Trained Emotion Classifiers for Deployment\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(\"Installation Instructions:\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(\"1. Download this zip file from Kaggle\\n\")\n",
    "        f.write(\"2. Extract the zip file\\n\")\n",
    "        f.write(\"3. Copy 'classifiers/' folder to 'deployment/models/'\\n\")\n",
    "        f.write(\"4. Start the backend server: python deployment/backend/main.py\\n\\n\")\n",
    "        f.write(\"Model Information:\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"Exported: {exported_count} classifier models\\n\")\n",
    "        f.write(f\"Categories: {', '.join([c.strip() for c in CATEGORIES])}\\n\")\n",
    "        f.write(f\"Model type: MLP (Multi-Layer Perceptron)\\n\")\n",
    "        f.write(f\"Framework: PyTorch\\n\\n\")\n",
    "        f.write(\"Server Requirements:\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(\"- GPU: RTX 4000 or equivalent (20GB VRAM recommended)\\n\")\n",
    "        f.write(\"- Python: 3.8+\\n\")\n",
    "        f.write(\"- PyTorch: 2.1.2+\\n\")\n",
    "        f.write(\"- Transformers: 4.37.0+\\n\")\n",
    "        f.write(\"- See deployment/backend/requirements.txt for full dependencies\\n\")\n",
    "    \n",
    "    # Create zip file\n",
    "    print(f\"\\nCreating zip archive: {OUTPUT_ZIP}\")\n",
    "    with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file_path in EXPORT_DIR.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                arcname = file_path.relative_to(EXPORT_DIR)\n",
    "                zipf.write(file_path, arcname)\n",
    "                print(f\"  ✓ Added: {arcname}\")\n",
    "    \n",
    "    archive_size_mb = OUTPUT_ZIP.stat().st_size / (1024**2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPORT COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"✓ Archive: {OUTPUT_ZIP}\")\n",
    "    print(f\"✓ Size: {archive_size_mb:.1f} MB\")\n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(\"1. Download 'deployment_models.zip' from Kaggle output\")\n",
    "    print(\"2. Extract to your local deployment folder\")\n",
    "    print(\"3. Follow deployment/README.md for server setup\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Cleanup temporary directory\n",
    "    shutil.rmtree(EXPORT_DIR)\n",
    "    \n",
    "    return OUTPUT_ZIP\n",
    "\n",
    "# Run export\n",
    "export_zip = export_models_for_deployment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 Facial Data Integration Summary\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "**Facial Data Structure:**\n",
    "- **5,532 total images** across 5 emotion categories\n",
    "- **Format:** 256×256 RGB images as flattened pixel arrays (196,608 values per row)\n",
    "- **No headers/labels:** Emotion determined by CSV filename\n",
    "\n",
    "**Distribution:**\n",
    "```\n",
    "Boredom      (boring.csv)    : 1,931 images\n",
    "Confusion    (confused.csv)  : 1,177 images  \n",
    "Engagement   (happiness.csv) :   593 images\n",
    "Neutral      (neutral.csv)   : 1,612 images\n",
    "Surprise     (surprise.csv)  :   219 images\n",
    "```\n",
    "\n",
    "### How It Addresses DAiSEE Imbalance\n",
    "\n",
    "**DAiSEE Problem:** Very few high-intensity samples for Boredom, Confusion, Frustration\n",
    "\n",
    "**Facial Data Solution:**\n",
    "- Adds **1,931 Boredom** images (assigned level 2)\n",
    "- Adds **1,177 Confusion** images (assigned level 2)\n",
    "- Adds **593 Engagement** images (assigned level 2)\n",
    "- Provides diverse facial expressions to improve classifier robustness\n",
    "\n",
    "**Combined Training:**\n",
    "- DAiSEE embeddings: Video-level temporal features\n",
    "- Facial embeddings: Single-frame static features\n",
    "- Both use same Qwen2.5-VL embedding space\n",
    "- Unified training improves generalization\n",
    "\n",
    "### Workflow Steps\n",
    "\n",
    "1. **Enable Facial Data:**\n",
    "   ```python\n",
    "   FACIAL_DATA_ENABLED = True\n",
    "   FACIAL_DATA_PATH = \"/kaggle/input/facial-data\"  # Update path\n",
    "   ```\n",
    "\n",
    "2. **Extract Embeddings:** (if `SKIP_EXTRACTION=False`)\n",
    "   - Loads pixel arrays from CSV\n",
    "   - Reshapes to 256×256×3 images\n",
    "   - Extracts embeddings with Qwen2.5-VL\n",
    "   - Saves to `FACIAL_EMBEDDINGS_DIR`\n",
    "\n",
    "3. **Train with Augmentation:**\n",
    "   - `EmbeddingDataset` combines DAiSEE + Facial samples\n",
    "   - Classifier sees both video and image embeddings\n",
    "   - Balanced training across intensity levels\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **No Frustration data** available in facial dataset\n",
    "- All facial samples assigned **intensity level 2** (moderate)\n",
    "- Could implement stratified sampling for different intensity levels if needed\n",
    "- Facial embeddings stored separately from DAiSEE embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔧 Bug Fixes Applied\n",
    "\n",
    "### Issue: \"Dataset: 0 valid samples\" Error\n",
    "\n",
    "**Problem:** The SKIP_EXTRACTION logic was copying embedding files but not properly tracking the copies, resulting in:\n",
    "- Embedding maps showing correct counts\n",
    "- But files not being accessible or paths being incorrect\n",
    "- Dataset initialization finding 0 valid samples\n",
    "\n",
    "**Fixes Applied:**\n",
    "\n",
    "1. **Better Copy Tracking** (Cell after \"SKIPPING EXTRACTION\"):\n",
    "   - Added `copied_count` and `skipped_count` counters\n",
    "   - Properly updates `new_map` only when files exist\n",
    "   - Shows detailed output: total embeddings, newly copied, already existed\n",
    "\n",
    "2. **Embedding Map Validation** (train_classifier function):\n",
    "   - Checks if embedding map files exist before loading\n",
    "   - Validates that mapped embedding files actually exist on disk\n",
    "   - Shows warnings for missing files with examples\n",
    "\n",
    "3. **Dataset Debugging** (EmbeddingDataset.__init__):\n",
    "   - Detects when 0 valid samples are found\n",
    "   - Shows diagnostic info: DataFrame size, embedding map size, sample ClipIDs\n",
    "   - Helps identify ClipID mismatches or path issues\n",
    "\n",
    "**Expected Output Now:**\n",
    "```\n",
    "TRAIN: Processing 4852 embeddings...\n",
    "  ✓ Total embeddings in map: 4852\n",
    "  ✓ Newly copied: 4852 (or 0 if already existed)\n",
    "  ✓ Already existed: 0 (or 4852)\n",
    "  ✓ Saved map to /kaggle/working/embeddings/train/embedding_map.pkl\n",
    "```\n",
    "\n",
    "**If Error Persists:**\n",
    "- Check that EXISTING_EMBEDDINGS_DIR path is correct\n",
    "- Verify embedding files exist: `/kaggle/input/qwen-daisee-embeddings/embeddings/train/*.npy`\n",
    "- Ensure ClipIDs in CSV match those in embedding_map.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to diagnose embedding issues\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check existing embeddings directory\n",
    "print(f\"\\n1. Checking EXISTING_EMBEDDINGS_DIR: {EXISTING_EMBEDDINGS_DIR}\")\n",
    "if EXISTING_EMBEDDINGS_DIR.exists():\n",
    "    print(f\"   ✓ Directory exists\")\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_dir = EXISTING_EMBEDDINGS_DIR / split\n",
    "        if split_dir.exists():\n",
    "            npy_files = list(split_dir.glob('*.npy'))\n",
    "            map_file = split_dir / 'embedding_map.pkl'\n",
    "            print(f\"   {split:12s}: {len(npy_files):5d} .npy files, map exists: {map_file.exists()}\")\n",
    "            \n",
    "            if map_file.exists():\n",
    "                with open(map_file, 'rb') as f:\n",
    "                    emap = pickle.load(f)\n",
    "                print(f\"                Map has {len(emap)} entries\")\n",
    "                # Check if any files in map exist\n",
    "                existing = sum(1 for p in emap.values() if Path(p).exists())\n",
    "                print(f\"                {existing}/{len(emap)} paths in map actually exist\")\n",
    "else:\n",
    "    print(f\"   ✗ Directory NOT found\")\n",
    "\n",
    "# Check working embeddings directory\n",
    "print(f\"\\n2. Checking EMBEDDINGS_DIR (working): {EMBEDDINGS_DIR}\")\n",
    "if EMBEDDINGS_DIR.exists():\n",
    "    print(f\"   ✓ Directory exists\")\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_dir = EMBEDDINGS_DIR / split\n",
    "        if split_dir.exists():\n",
    "            npy_files = list(split_dir.glob('*.npy'))\n",
    "            map_file = split_dir / 'embedding_map.pkl'\n",
    "            print(f\"   {split:12s}: {len(npy_files):5d} .npy files, map exists: {map_file.exists()}\")\n",
    "            \n",
    "            if map_file.exists():\n",
    "                with open(map_file, 'rb') as f:\n",
    "                    emap = pickle.load(f)\n",
    "                print(f\"                Map has {len(emap)} entries\")\n",
    "else:\n",
    "    print(f\"   ✗ Directory NOT found (will be created)\")\n",
    "\n",
    "# Check label DataFrames\n",
    "print(f\"\\n3. Checking Label DataFrames:\")\n",
    "print(f\"   Train:      {len(train_df)} rows\")\n",
    "print(f\"   Validation: {len(val_df)} rows\")\n",
    "print(f\"   Test:       {len(test_df)} rows\")\n",
    "print(f\"   Sample ClipID: {train_df.iloc[0]['ClipID']}\")\n",
    "\n",
    "# Check facial data if enabled\n",
    "if FACIAL_DATA_ENABLED:\n",
    "    print(f\"\\n4. Checking Facial Data:\")\n",
    "    print(f\"   FACIAL_DATA_PATH: {FACIAL_DATA_PATH}\")\n",
    "    facial_path = Path(FACIAL_DATA_PATH)\n",
    "    if facial_path.exists():\n",
    "        print(f\"   ✓ Directory exists\")\n",
    "        csv_files = list(facial_path.glob('*.csv'))\n",
    "        print(f\"   Found {len(csv_files)} CSV files\")\n",
    "        for csv in csv_files[:5]:\n",
    "            print(f\"      - {csv.name}\")\n",
    "    else:\n",
    "        print(f\"   ✗ Directory NOT found\")\n",
    "    \n",
    "    print(f\"\\n   Loaded facial_data_dfs: {len(facial_data_dfs)} categories\")\n",
    "    for cat, df in facial_data_dfs.items():\n",
    "        print(f\"      {cat}: {len(df)} images\")\n",
    "    \n",
    "    if facial_embedding_maps:\n",
    "        print(f\"\\n   Loaded facial_embedding_maps: {len(facial_embedding_maps)} categories\")\n",
    "        for cat, emap in facial_embedding_maps.items():\n",
    "            print(f\"      {cat}: {len(emap)} embeddings\")\n",
    "else:\n",
    "    print(f\"\\n4. Facial Data: DISABLED\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Diagnostic Cell (Run if you encounter errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ Extraction Configuration Guide\n",
    "\n",
    "### Configuration Flags\n",
    "\n",
    "**Main Workflow Control:**\n",
    "- `SKIP_EXTRACTION` - Skip ALL extraction, use existing embeddings only\n",
    "  - `True`: Training only mode (fastest, requires pre-extracted embeddings)\n",
    "  - `False`: Enable extraction (choose what to extract below)\n",
    "\n",
    "**Extraction Targets** (only used when `SKIP_EXTRACTION = False`):\n",
    "- `EXTRACT_DAISEE` - Extract DAiSEE video embeddings\n",
    "  - `True`: Extract embeddings from DAiSEE videos\n",
    "  - `False`: Use existing DAiSEE embeddings\n",
    "  \n",
    "- `EXTRACT_FACIAL` - Extract facial data embeddings\n",
    "  - `True`: Extract embeddings from facial data CSVs (requires `FACIAL_DATA_ENABLED = True`)\n",
    "  - `False`: Use existing facial embeddings or skip facial data\n",
    "\n",
    "### Common Scenarios\n",
    "\n",
    "**1. Training Only (fastest - use existing embeddings):**\n",
    "```python\n",
    "SKIP_EXTRACTION = True\n",
    "FACIAL_DATA_ENABLED = True  # If you want to use facial data\n",
    "# EXTRACT_DAISEE and EXTRACT_FACIAL are ignored\n",
    "```\n",
    "\n",
    "**2. Extract DAiSEE Only:**\n",
    "```python\n",
    "SKIP_EXTRACTION = False\n",
    "EXTRACT_DAISEE = True\n",
    "EXTRACT_FACIAL = False\n",
    "FACIAL_DATA_ENABLED = False  # Optional\n",
    "```\n",
    "\n",
    "**3. Extract Facial Data Only:**\n",
    "```python\n",
    "SKIP_EXTRACTION = False\n",
    "EXTRACT_DAISEE = False      # Use existing DAiSEE embeddings\n",
    "EXTRACT_FACIAL = True\n",
    "FACIAL_DATA_ENABLED = True  # Required\n",
    "```\n",
    "\n",
    "**4. Extract Both (slowest - full extraction):**\n",
    "```python\n",
    "SKIP_EXTRACTION = False\n",
    "EXTRACT_DAISEE = True\n",
    "EXTRACT_FACIAL = True\n",
    "FACIAL_DATA_ENABLED = True\n",
    "```\n",
    "\n",
    "**5. Extract DAiSEE, Use Existing Facial:**\n",
    "```python\n",
    "SKIP_EXTRACTION = False\n",
    "EXTRACT_DAISEE = True\n",
    "EXTRACT_FACIAL = False\n",
    "FACIAL_DATA_ENABLED = True  # Will load existing facial embeddings\n",
    "```\n",
    "\n",
    "### Time Estimates\n",
    "\n",
    "Approximate extraction times on T4 GPU:\n",
    "- **DAiSEE (8,571 videos):** ~2-3 hours\n",
    "- **Facial Data (5,532 images):** ~30-45 minutes\n",
    "- **Training (all classifiers):** ~10-15 minutes\n",
    "\n",
    "**Tip:** Extract once, then set `SKIP_EXTRACTION = True` for all subsequent training runs!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4221070,
     "sourceId": 7280115,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9127099,
     "sourceId": 14301756,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9143258,
     "sourceId": 14322485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9154507,
     "sourceId": 14338316,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
